{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other questions\n",
    "\n",
    "Perguntas de amigos. =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Em redes neurais, como é feita a inicialização do *bias* e pesos?\n",
    "\n",
    "A inicialização do *bias* normalmente é 0 e dos pesos é aleatória entre 0 e 1. Porém, devem haver métodos que inicalizem de maneira melhor esses parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qual a diferença entre o f1 micro e do f1 macro?\n",
    "\n",
    "Para um problema de classificação multiclasse, a **f1 micro** é a média harmônica entre a precisão (micro) e a revocação (micro) e a **f1 macro** é a média aritimética das f1 de cada classe.\n",
    "\n",
    "- **f1 micro**:\n",
    "\n",
    "$$f_{1 micro} = \\frac{2*Precisão_{micro}*Recall_{micro}}{Precisão_{micro}+Recall_{micro}}$$\n",
    "\n",
    "- - **f1 macro**:\n",
    "\n",
    "$$ f_{1 macro} = \\frac{1}{elementos} \\sum F_{1j}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é o p-valor?\n",
    "\n",
    "***p-valor*** é um valor constítuido de três partes:\n",
    "\n",
    "1. A probabilidade do evento de interesse ocorrer aleatoriamente;\n",
    "2. A probabilidade de um evento similar ocorrer; e\n",
    "3. A probabilidade de eventos mais raros ocorrerem.\n",
    "\n",
    "O somatório dessas três partes, o p-valor, é usado para comparar com um limiar de decisão, usualmente 5% (nível de significância) e inferirmos se o evento de interesse pode ter ocorrido por algum motivo além da aleatoriedade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como tratar categóricos em modelos de agrupamento?\n",
    "\n",
    "A princípio, eu transformaria essa variável em numérica. E, se houvesse alguma relação de crescimento ou decrescimento entre elas, optaria pela transformação delas em números inteiro sequenciais. Agora, se não houver uma relação de crescimento ou decrescimento entre elas, ainda não sei como tratar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nomeie algumas funções de ativação comuns e explique quando usar.\n",
    "\n",
    "> Da referência (1), temos:\n",
    "> Quando aplicadas para conferir não-linearidade ao modelo, as funções de ativação devem ter idealmente algumas características:\n",
    "> \n",
    "> **Custo computacional:** É bom lembrar que operações matemáticas lineares são computacionalmente baratas. Portanto, uma função de ativação deve incrementar esse custo apenas na medida em que confere a característica desejada. Isso é especialmente relevante já que muitas funções de ativação são elas mesmas não-lineares. Em redes neurais grandes que serão treinadas por muitas épocas, o custo computacional adicional pode ter um impacto a se considerar.\n",
    ">\n",
    "> **Diferenciabilidade:** Durante o treinamento das redes neurais, seus pesos e biases são ajustados pelo mecanismo de descida do gradiente, que exige que cada operação matemática realizada em cada camada tenha sua derivada calculada. Portanto, essas operações devem ser diferenciáveis. Sem cumprir esse requisito, uma função não pode ser usada para ativação em uma rede neural.\n",
    ">\n",
    "> **Centradas em zero:** Isso quer dizer que o universo de resultados que a função de ativação é capaz de produzir deve ter iguais probabilidades de ser positivo ou negativo. Isso se deve ao fato de que, no cálculo do gradiente, o resultado da função de ativação é utilizado para determinar o valor do gradiente das operações anteriores. Se esse resultado for apenas positivo ou apenas negativo (ou seja, não centrado em zero), o valor do gradiente também será sempre positivo ou negativo, e os ajustes nos pesos e biases seguirão sempre uma única direção. Isso pode fazer com que a rede neural tenha dificuldades para convergir.\n",
    ">\n",
    "> **Não produzir platôs:** Platôs são regiões do universo de resultados que tendem a ser constantes. Quando uma função tem regiões constantes, a derivada nessas regiões tende a zero, o que causa o problema chamado de vanishing gradient, ou seja, o gradiente tende a zero e a rede não é mais capaz de convergir.\n",
    ">\n",
    "> Alguns exemplos de funções:\n",
    ">\n",
    "> | Função de ativação | Gráfico e sua derivada | Camadas onde pode ser aplicada | Observações |\n",
    "> |:-|:-:|:-|:-|\n",
    "> |**Degrau<br>(step)**||||\n",
    "> |**Sigmóide<br> ou Logística**||- Saída||\n",
    "> |**Tangente hiperbólica<br>(tanh)**||||\n",
    "> |**Softmax**||- Saída||\n",
    "> |**ReLU**||- Intermediárias||\n",
    "> |**Leaky ReLU**||- Intermediárias||\n",
    "\n",
    "Fontes:\n",
    "1. Post da [IA Expert](https://iaexpert.academy/2020/05/25/funcoes-de-ativacao-definicao-caracteristicas-e-quando-usar-cada-uma/).\n",
    "2. Post do [Matheus Facure](https://matheusfacure.github.io/2017/07/12/activ-func/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
